{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T05:59:37.048824Z",
     "start_time": "2025-12-11T05:59:35.313624Z"
    }
   },
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv_path='secrets.env')\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. Setup Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# 2. Define Dummy Data\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"LangChain provides abstractions to make working with LLMs easy.\",\n",
    "        metadata={\"source\": \"documentation\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Elasticsearch is a distributed, RESTful search and analytics engine.\",\n",
    "        metadata={\"source\": \"documentation\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Hybrid search combines vector and keyword search for better results.\",\n",
    "        metadata={\"source\": \"concept\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "INDEX_NAME = 'my-index'\n",
    "PIPELINE_ID = 'my-rrf-pipeline'\n",
    "\n",
    "# 3. Create the vector store\n",
    "vector_store = OpenSearchVectorSearch(\n",
    "    embedding_function=embeddings,\n",
    "    index_name=INDEX_NAME,\n",
    "    opensearch_url=\"https://localhost:9200\",\n",
    "    engine=\"faiss\",\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False,\n",
    "    http_auth=(\"admin\", \"StrongPassword123!\")\n",
    ")\n",
    "\n",
    "# 4. Create the RRF Search Pipeline\n",
    "# We use the specific 'search_pipeline' client namespace\n",
    "pipeline_body = {\n",
    "    \"description\": \"Post-processor for hybrid search using RRF\",\n",
    "    \"phase_results_processors\": [\n",
    "        {\n",
    "            \"score-ranker-processor\": {\n",
    "                \"combination\": {\n",
    "                    \"technique\": \"rrf\",\n",
    "                    \"rank_constant\": 60\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Creating/Updating pipeline: {PIPELINE_ID}...\")\n",
    "vector_store.client.search_pipeline.put(\n",
    "    id=PIPELINE_ID,\n",
    "    body=pipeline_body\n",
    ")\n",
    "\n",
    "# 5. Load Documents into OpenSearch\n",
    "# This embeds the documents using your GoogleGenerativeAIEmbeddings and indexes them\n",
    "print(\"Indexing documents...\")\n",
    "ids = vector_store.add_documents(docs)\n",
    "print(f\"Indexed {len(ids)} documents.\")\n",
    "\n",
    "# Optional: Force a refresh so documents are immediately searchable\n",
    "vector_store.client.indices.refresh(index=INDEX_NAME)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating/Updating pipeline: my-rrf-pipeline...\n",
      "Indexing documents...\n",
      "Indexed 3 documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T05:59:37.121914Z",
     "start_time": "2025-12-11T05:59:37.108332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class OpenSearchRRFRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    A custom retriever that uses an OpenSearch Search Pipeline\n",
    "    to perform Hybrid Search (Vector + Keyword) with RRF.\n",
    "    \"\"\"\n",
    "    vector_store: OpenSearchVectorSearch\n",
    "    pipeline_id: str\n",
    "    k: int = 5\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "\n",
    "        # 1. Generate the embedding for the user's query\n",
    "        # We reuse the embedding function defined in the vector store\n",
    "        query_vector = self.vector_store.embedding_function.embed_query(query)\n",
    "\n",
    "        # 2. Construct the OpenSearch \"Hybrid\" query\n",
    "        # This mirrors the logic required by the 'score-ranker-processor'\n",
    "        query_body = {\n",
    "            \"_source\": {\"exclude\": [\"vector_field\"]},\n",
    "            \"size\": self.k,\n",
    "            \"query\": {\n",
    "                \"hybrid\": {\n",
    "                    \"queries\": [\n",
    "                        # Clause 1: Keyword Match\n",
    "                        {\n",
    "                            \"match\": {\n",
    "                                \"text\": {\n",
    "                                    \"query\": query\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        # Clause 2: Vector k-NN\n",
    "                        {\n",
    "                            \"knn\": {\n",
    "                                \"vector_field\": {\n",
    "                                    \"vector\": query_vector,\n",
    "                                    \"k\": self.k\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 3. Execute the search via the low-level OpenSearch client\n",
    "        response = self.vector_store.client.search(\n",
    "            index=self.vector_store.index_name,\n",
    "            body=query_body,\n",
    "            params={\"search_pipeline\": self.pipeline_id}\n",
    "        )\n",
    "\n",
    "        # 4. Map results back to LangChain Documents\n",
    "        results = []\n",
    "        for hit in response[\"hits\"][\"hits\"]:\n",
    "            content = hit[\"_source\"].get(\"text\", \"\")\n",
    "            metadata = hit[\"_source\"].get(\"metadata\", {})\n",
    "\n",
    "            # Capture the RRF score in metadata for debugging/ranking\n",
    "            metadata[\"rrf_score\"] = hit[\"_score\"]\n",
    "\n",
    "            results.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "        return results"
   ],
   "id": "ef3f79abcad4229a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T06:02:45.646103Z",
     "start_time": "2025-12-11T06:02:42.952687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate your custom retriever\n",
    "retriever = OpenSearchRRFRetriever(\n",
    "    vector_store=vector_store,\n",
    "    pipeline_id=PIPELINE_ID,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# --- Example 1: Direct Usage ---\n",
    "results = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "print(f\"Top Result: {results[0].page_content}\")\n",
    "print(f\"RRF Score: {results[0].metadata['rrf_score']}\")\n",
    "\n",
    "\n",
    "# --- Example 2: Use in a RAG Chain (LCEL) ---\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Setup a simple RAG chain\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question based only on the context provided:\\n\\nContext: {context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = rag_chain.invoke(\"How does hybrid search work?\")\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response)"
   ],
   "id": "adf7f46deed427e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Result: LangChain provides abstractions to make working with LLMs easy.\n",
      "RRF Score: 0.032786883\n",
      "\n",
      "LLM Response:\n",
      "Hybrid search works by combining vector and keyword search.\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
